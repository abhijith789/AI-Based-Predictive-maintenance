{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f1424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABHIJITH\\AppData\\Local\\Temp\\ipykernel_1524\\686984688.py:41: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"machine_id\", group_keys=False).apply(add_failure_horizon_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution (fail_within_24h):\n",
      "fail_within_24h\n",
      "0    430020\n",
      "1      2030\n",
      "Name: count, dtype: int64\n",
      "Rows after dropping initial warm-up period: 424900\n",
      "Original train distribution:\n",
      "fail_within_24h\n",
      "0    338790\n",
      "1      1160\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced train distribution:\n",
      "fail_within_24h\n",
      "0    5800\n",
      "1    1160\n",
      "Name: count, dtype: int64\n",
      "        machine_id           timestamp  fail_within_24h  proba_24h\n",
      "361423          41 2024-02-19 14:20:00                0      0.895\n",
      "361438          41 2024-02-19 16:50:00                0      0.895\n",
      "361437          41 2024-02-19 16:40:00                0      0.895\n",
      "361424          41 2024-02-19 14:30:00                0      0.885\n",
      "361420          41 2024-02-19 13:50:00                0      0.885\n",
      "\n",
      "Classification report with threshold=0.20:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.910     0.951     84080\n",
      "           1      0.057     0.526     0.103       870\n",
      "\n",
      "    accuracy                          0.906     84950\n",
      "   macro avg      0.526     0.718     0.527     84950\n",
      "weighted avg      0.985     0.906     0.942     84950\n",
      "\n",
      "Confusion matrix:\n",
      "[[76527  7553]\n",
      " [  412   458]]\n",
      "\n",
      "Saved model to pd_24h_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. LOAD DATA\n",
    "# -----------------------------------\n",
    "df = pd.read_csv(\"synthetic_sensor_data.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# Ensure correct ordering\n",
    "df = df.sort_values([\"machine_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. CREATE TARGET: fail_within_24h\n",
    "# -----------------------------------\n",
    "HORIZON_HOURS = 24\n",
    "\n",
    "def add_failure_horizon_labels(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    group = group.sort_values(\"timestamp\").copy()\n",
    "    \n",
    "    group[\"failure_time\"] = group[\"timestamp\"].where(group[\"failed\"] == 1)\n",
    "    group[\"next_failure_time\"] = group[\"failure_time\"].bfill()\n",
    "    \n",
    "    time_diff = group[\"next_failure_time\"] - group[\"timestamp\"]\n",
    "    group[\"time_to_next_failure_hours\"] = time_diff.dt.total_seconds() / 3600\n",
    "    \n",
    "    group[\"fail_within_24h\"] = (\n",
    "        (group[\"time_to_next_failure_hours\"] >= 0) &\n",
    "        (group[\"time_to_next_failure_hours\"] <= HORIZON_HOURS)\n",
    "    ).astype(int)\n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby(\"machine_id\", group_keys=False).apply(add_failure_horizon_labels)\n",
    "\n",
    "print(df[\"failed\"].value_counts())\n",
    "print(df[\"fail_within_24h\"].value_counts())\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. FEATURE ENGINEERING (rolling stats)\n",
    "# -----------------------------------\n",
    "# We have data every 10 minutes:\n",
    "FREQ_MIN = 10\n",
    "STEPS_PER_HOUR = int(60 / FREQ_MIN)    # 6\n",
    "WINDOW_6H = 6 * STEPS_PER_HOUR         # 36\n",
    "WINDOW_24H = 24 * STEPS_PER_HOUR       # 144\n",
    "\n",
    "sensor_cols = [\"temp_c\", \"vibration_ms2\", \"pressure_psi\", \"load_pct\", \"rpm\"]\n",
    "\n",
    "df = df.sort_values([\"machine_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "grouped = df.groupby(\"machine_id\", group_keys=False)\n",
    "\n",
    "for col in sensor_cols:\n",
    "    # 6-hour rolling stats\n",
    "    df[f\"{col}_mean_6h\"] = grouped[col].rolling(window=WINDOW_6H, min_periods=WINDOW_6H).mean().reset_index(level=0, drop=True)\n",
    "    df[f\"{col}_std_6h\"]  = grouped[col].rolling(window=WINDOW_6H, min_periods=WINDOW_6H).std().reset_index(level=0, drop=True)\n",
    "    df[f\"{col}_max_6h\"]  = grouped[col].rolling(window=WINDOW_6H, min_periods=WINDOW_6H).max().reset_index(level=0, drop=True)\n",
    "    \n",
    "    # 24-hour rolling stats\n",
    "    df[f\"{col}_mean_24h\"] = grouped[col].rolling(window=WINDOW_24H, min_periods=WINDOW_24H).mean().reset_index(level=0, drop=True)\n",
    "    df[f\"{col}_std_24h\"]  = grouped[col].rolling(window=WINDOW_24H, min_periods=WINDOW_24H).std().reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Trend features (delta from 24h mean)\n",
    "    df[f\"{col}_delta_24h\"] = df[col] - df[f\"{col}_mean_24h\"]\n",
    "\n",
    "# Example spike feature for temperature: count of high-temp points in last 24h\n",
    "df[\"temp_high\"] = (df[\"temp_c\"] > 80).astype(int)\n",
    "df[\"temp_high_count_24h\"] = grouped[\"temp_high\"].rolling(window=WINDOW_24H, min_periods=WINDOW_24H).sum().reset_index(level=0, drop=True)\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. CLEAN UP & DROP EARLY ROWS WITHOUT FULL HISTORY\n",
    "# -----------------------------------\n",
    "# Drop rows where 24h features are NaN (initial warm-up period per machine)\n",
    "feature_cols = [c for c in df.columns if any(k in c for k in [\"mean_6h\", \"std_6h\", \"max_6h\", \"mean_24h\", \"std_24h\", \"delta_24h\", \"temp_high_count_24h\"])]\n",
    "\n",
    "df_model = df.dropna(subset=feature_cols + [\"fail_within_24h\"]).copy()\n",
    "\n",
    "print(\"Rows after dropping initial warm-up period:\", len(df_model))\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. TRAIN-TEST SPLIT (TIME-BASED)\n",
    "# -----------------------------------\n",
    "time_threshold = df_model[\"timestamp\"].quantile(0.8)\n",
    "\n",
    "train = df_model[df_model[\"timestamp\"] <= time_threshold]\n",
    "test  = df_model[df_model[\"timestamp\"] > time_threshold]\n",
    "\n",
    "print(\"Original train distribution:\")\n",
    "print(train[\"fail_within_24h\"].value_counts())\n",
    "\n",
    "# -----------------------------------\n",
    "# 5B. FIX CLASS IMBALANCE (UNDERSAMPLE MAJORITY)\n",
    "# -----------------------------------\n",
    "train_pos = train[train[\"fail_within_24h\"] == 1]\n",
    "train_neg = train[train[\"fail_within_24h\"] == 0]\n",
    "\n",
    "# Undersample majority class to ratio (pos : neg = 1 : 5)\n",
    "neg_sample_size = min(len(train_neg), len(train_pos) * 5)\n",
    "train_neg_sampled = train_neg.sample(n=neg_sample_size, random_state=42)\n",
    "\n",
    "train_balanced = pd.concat([train_pos, train_neg_sampled], axis=0)\n",
    "train_balanced = train_balanced.sample(frac=1.0, random_state=42)  # shuffle\n",
    "\n",
    "X_train = train_balanced[feature_cols]\n",
    "y_train = train_balanced[\"fail_within_24h\"]\n",
    "\n",
    "X_test = test[feature_cols]\n",
    "y_test = test[\"fail_within_24h\"]\n",
    "\n",
    "print(\"\\nBalanced train distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# -----------------------------------\n",
    "# 6. TRAIN MODEL (NO class_weight needed now)\n",
    "# -----------------------------------\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------------\n",
    "# 7. EVALUATE WITH CUSTOM THRESHOLD\n",
    "# -----------------------------------\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Lower threshold improves recall (important for failure prediction)\n",
    "THRESHOLD = 0.20\n",
    "y_pred = (y_proba >= THRESHOLD).astype(int)\n",
    "\n",
    "# Attach probabilities back to the test dataframe\n",
    "test_with_proba = test.copy()\n",
    "test_with_proba[\"proba_24h\"] = y_proba\n",
    "test_with_proba[\"pred_24h\"] = y_pred\n",
    "\n",
    "# Sort by highest predicted probability\n",
    "top_risky = test_with_proba.sort_values(\"proba_24h\", ascending=False).head(5)\n",
    "print(top_risky[[\"machine_id\", \"timestamp\", \"fail_within_24h\", \"proba_24h\"]])\n",
    "\n",
    "print(\"\\nClassification report with threshold=0.20:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# -----------------------------------\n",
    "# 8. SAVE MODEL FOR API/APP\n",
    "# -----------------------------------\n",
    "model_artifact = {\n",
    "    \"model\": clf,\n",
    "    \"feature_cols\": feature_cols\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifact, \"pd_24h_model.joblib\")\n",
    "print(\"\\nSaved model to pd_24h_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8200a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top risky sample probability: 0.895\n",
      "Label (fail_within_24h): 0\n",
      "{\n",
      "  \"features\": {\n",
      "    \"temp_c_mean_6h\": 59.658417932856224,\n",
      "    \"temp_c_std_6h\": 1.227856431125955,\n",
      "    \"temp_c_max_6h\": 61.83235335345177,\n",
      "    \"temp_c_mean_24h\": 59.7622625791732,\n",
      "    \"temp_c_std_24h\": 1.4584602959242088,\n",
      "    \"temp_c_delta_24h\": -0.05232565141596979,\n",
      "    \"vibration_ms2_mean_6h\": 1.6945291476061946,\n",
      "    \"vibration_ms2_std_6h\": 0.21788714455398325,\n",
      "    \"vibration_ms2_max_6h\": 2.1976394144275138,\n",
      "    \"vibration_ms2_mean_24h\": 1.6536826792463808,\n",
      "    \"vibration_ms2_std_24h\": 0.2102923553458617,\n",
      "    \"vibration_ms2_delta_24h\": -0.3635887987839963,\n",
      "    \"pressure_psi_mean_6h\": 292.4262603557566,\n",
      "    \"pressure_psi_std_6h\": 5.344716665226278,\n",
      "    \"pressure_psi_max_6h\": 302.9737722082797,\n",
      "    \"pressure_psi_mean_24h\": 291.53699760743785,\n",
      "    \"pressure_psi_std_24h\": 4.695917341293183,\n",
      "    \"pressure_psi_delta_24h\": -1.6470987837406028,\n",
      "    \"load_pct_mean_6h\": 62.26964684641051,\n",
      "    \"load_pct_std_6h\": 5.117594154391088,\n",
      "    \"load_pct_max_6h\": 71.48321325995312,\n",
      "    \"load_pct_mean_24h\": 61.838695994015474,\n",
      "    \"load_pct_std_24h\": 5.31240842155831,\n",
      "    \"load_pct_delta_24h\": -4.905483010504554,\n",
      "    \"rpm_mean_6h\": 1598.9512856549995,\n",
      "    \"rpm_std_6h\": 47.28152581491866,\n",
      "    \"rpm_max_6h\": 1702.958720054287,\n",
      "    \"rpm_mean_24h\": 1600.1019653855542,\n",
      "    \"rpm_std_24h\": 43.136515315205116,\n",
      "    \"rpm_delta_24h\": -36.1065515739524,\n",
      "    \"temp_high_count_24h\": 0.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Pick the most risky sample\n",
    "row = top_risky.iloc[0]\n",
    "\n",
    "# Build the feature dict in the same order as feature_cols\n",
    "feature_payload = {col: float(row[col]) for col in feature_cols}\n",
    "\n",
    "print(\"Top risky sample probability:\", row[\"proba_24h\"])\n",
    "print(\"Label (fail_within_24h):\", row[\"fail_within_24h\"])\n",
    "\n",
    "# Print JSON for FastAPI\n",
    "json_payload = {\"features\": feature_payload}\n",
    "print(json.dumps(json_payload, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
